# 线性回归

线性回归属于算法组件中机器学习的一种回归组件。可以在“公共组件-算法设计-机器学习-回归”的分类中找到它。

## 原理

在统计学中，线性回归是一种建模因变量与一个或多个解释变量(或自变量)之间关系的线性方法。一个解释变量的情况称为简单线性回归。对于一个以上的解释变量，这个过程称为多元线性回归。这一项不同于多变量线性回归，多变量线性回归预测多个因变量，而不是单个因变量。

在线性回归中，利用线性预测函数对关系进行建模，预测函数的未知模型参数由数据估计得到。这种模型称为线性模型。最常见的情况是，给定解释变量(或预测因子)的值的响应的条件平均值假定为这些值的仿射函数;不太常用的是条件中位数或其他分位数。与所有形式的回归分析一样，线性回归关注给定预测因子值响应的条件概率分布，而不是多元分析中的所有变量的联合概率分布。

线性回归是第一种需要严格研究并在实际应用中得到广泛应用的回归分析方法。这是因为线性依赖于其未知参数的模型比与参数非线性相关的模型更容易拟合，而且结果估计量的统计特性更容易确定。

线性回归有许多实际用途。可以分为以下两大类:

- 如果目标是预测，或减少误差，线性回归可用于建立预测模型，匹配响应值和解释变量的观测数据集之间的关系。建立该模型后，如果在没有相应响应值的情况下收集解释变量的附加值，则可以使用拟合模型对响应进行预测。
- 如果目的是将响应变量的变化归因于解释变量的变化,线性回归分析可以应用于量化响应变量和解释变量之间的关系的强度,特别是一些解释性变量与响应变量是否可能没有线性关系,或确定解释变量的子集可能包含的冗余信息。

线性回归模型通常使用最小二乘拟合的方法,但是也可以使用其他方法拟合,比如在其他范数的基础上最小化“失拟项”(如最小绝对偏差回归),或最小化最小二乘法增加惩罚项的代价函数像岭回归(L2范数惩罚项)和Lasso(L1范数惩罚项)。相反，最小二乘法可用于拟合非线性模型。因此，尽管术语“最小二乘”和“线性模型”是紧密联系的，但它们并不是同义词。

![](Untitled-6c31b5cf-eddd-45b6-bf3f-fc8f44ccb5f9.png)

## 使用

将线性回归模块拖出，并找到想要使用的数据集与模型预测组件，分别与线性回归组件连接。

![](Untitled-f33d7b54-8cf5-4eb7-99e2-caf093a1481f.png)

预测结束后可以点击模型预测的查看结果，可以看到最后的prediction为最终的预测结果。

![](Untitled-4f4a9df4-c8c6-4b8d-90eb-d87b27a3867e.png)

## 参数

点击线性回归组件的参数设置选项，可以看到有许多参数可以手动设置。

![](Untitled-bacfc2da-2c5d-4c67-8216-7b2c70612d25.png)

**Fit Intercept**（bool）:是否计算该模型的截距。

**normalize**（bool）:当fit_intercept设置为False时，将忽略该参数。若为真，则回归前对回归量X进行归一化处理，取均值减去后除以l2-范数。

**Copy X**（bool）:如果为真，则复制X;否则，它可能被覆盖。

**N Jobs**（int or None）:用于计算的作业数。

**需要训练**（bool）:该模型是否需要训练，默认为需要训练

## 参考文献

[sklearn.linear_model.LinearRegression - scikit-learn 0.21.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)

[线性回归_百度百科](https://baike.baidu.com/item/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/8190345?fr=aladdin)

[Linear regression](https://en.wikipedia.org/wiki/Linear_regression)